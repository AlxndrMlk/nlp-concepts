{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "layers = tf.keras.layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math behind attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau attention\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Bahdanau attention** is also known as *concat* or *additive* attention. It has been introduced in the seminal paper [***Neural machine translation by jointly learning to align and translate***](https://arxiv.org/pdf/1409.0473.pdf) by Bahdanau et al., 2015.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/509/1*O-xKW4z-HWg1AC0vVFe3vg.png\">\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<img src=\"https://sigmoidal.io/wp-content/uploads/2020/09/attention-heatmap-2.png.webp\" width=400>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "To understand  **Bahdanau attention** let's start with understanding the decoder function:\n",
    "\n",
    "\n",
    "$$ \\Large p(y_j|y_0, ..., y_{j-1}, \\mathbf{x}) = g(y_{j-1}, s_j, c_j)$$\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "**Let's unpack it!**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $g()$ is some non-linear function (e.g. an **RNN**)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $s_j$ is the decoder's **hidden state** at position $t$. It's a function of:\n",
    "\n",
    "\n",
    "$$\\Large s_j = g(s_{j-1}, y_{j - 1}, c_j)$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "...and $c_j$ is a **weighted sum** of **encoder hidden states**:\n",
    "\n",
    "\n",
    "ðŸ’ŽðŸ’ŽðŸ’Ž$$\\Large c_j = \\sum_i^{T_x}\\alpha_{i, j}h_i$$:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "...while $\\alpha$s are computed using **softmax** over $e_{i,j}$s\n",
    "\n",
    "$$\\Large \\alpha_{i, j} = \\frac{\\exp(e_{i,j})}{\\sum_{i'}^{T_x} \\exp(e_{i',j})}$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "...and $e_{i, j}$ is **alignment energy** defined as:\n",
    "\n",
    "$$\\Large e_{i, j} = NN(s_{j-1}, h_i)$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "where:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $h_i$ is the encoder's **hidden state** for the $i^{th}$ **input token**.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\"*The probability $\\alpha_{i,j}$, or its associated energy $e_{i,j}$, reflects the importance of the annotation $h_i$ with respect to the previous hidden state $s_j-1$ in deciding the next state $s_i$ and generating $y_i$. Intuitively,\n",
    "this implements a **mechanism of attention** in the decoder.*\"\n",
    "\n",
    "Bahdanau et al., 2015\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers - scaled dot product self-attention\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "where:\n",
    "\n",
    "* $Q$ is a **query** matrix $\\in \\mathbb{R}^{L_Q \\times D}$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $K$ is a **key** matrix $\\in \\mathbb{R}^{L_K \\times D}$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $D$ is the embedding dimensionality\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $L_X$ are sequence lengths (e.g. in the translation setting)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Let's stop here for a while and contemplate this sub-equation:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\\Large W_A = softmax(QK^T)$$\n",
    "\n",
    "where $W_A \\in \\mathbb{R}^{L_Q \\times L_K}$\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "Now, let's add the $V$ matrix:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\\Large Attention(Q, K, V) = softmax(QK^T)V$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* $V$ is a **value** matrix $\\in \\mathbb{R}^{L_K \\times D}$ \n",
    "\n",
    "...and is in fact pretty often the same matrix as $K$\n",
    "\n",
    "<br><br> \n",
    "\n",
    "Let's try to make sense out of this:\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src='content/att_2.jpg' width=700>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "You can also think about attention as a **soft dictionary** object:\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/e/ey_nosukeru/20190622/20190622045649.png\" width=600>\n",
    "\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "\n",
    "We're still missing one element. Let's get back to the full formula:\n",
    "\n",
    "\n",
    "$$\\Large Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "* $\\sqrt{d_k}$ is the embedding dimensinality.\n",
    "\n",
    "<br><br>\n",
    "<br><br>\n",
    "\n",
    "\"*We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.*\"\n",
    "\n",
    "Vaswani et al., 2017\n",
    "\n",
    "<br><br>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Causal self-attention\n",
    "\n",
    "In **causal self-attention** we mask the words before the current position. It's used in **generative models**.\n",
    "\n",
    "$$\\Large Attention(Q, K, V) = softmax(\\frac{QK^T + M}{\\sqrt{d_k}})V$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "where $M$ is a $L \\times L$ mask matrix with $0$s on and below the diagonal and $- \\infty$ above the diagonal.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going multihead\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src='content/multi.jpg' width=400>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\\Large Multihead(Q, K, V) = Concat(h_i, ..., h_m)W^O$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "$$\\Large h_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-dl-env]",
   "language": "python",
   "name": "conda-env-nlp-dl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
