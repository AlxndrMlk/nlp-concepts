{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this in Colab please uncomment and execute the line below\n",
    "# !pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import FastText, Word2Vec, Doc2Vec\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "import wordninja\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Concepts 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and cleans a provided string, removing all punctuation and lowercasing.\n",
    "    \n",
    "    Input: arbitrary text <str>\n",
    "    Returns: a <list> of tokens <str>\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_text = []\n",
    "    \n",
    "    tokenized = text_to_word_sequence(text)\n",
    "    splitted = [wordninja.split(string) for string in tokenized]\n",
    "    \n",
    "    for phrase in splitted:\n",
    "        clean_text += phrase\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(article, model, in_vocab_check = True):\n",
    "    emb_sum = 0\n",
    "    n_elems = 0\n",
    "    \n",
    "    for word in article:\n",
    "        \n",
    "        if in_vocab_check:\n",
    "            \n",
    "            if word in model.wv.vocab:\n",
    "                emb_sum += model.wv[word]\n",
    "                n_elems += 1\n",
    "        else:\n",
    "            emb_sum += model.wv[word]\n",
    "            n_elems += 1\n",
    "        \n",
    "    return emb_sum / n_elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'.\\data\\SMSSpamCollection.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data_tuples = []\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_splt = line.split('\\t')\n",
    "        data_tuples.append((line_splt[0], line_splt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "data = pd.DataFrame(data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update colnames\n",
    "data.columns = ['label', 'content']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast labels to ints\n",
    "data['label'] = data.label.apply(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'][5083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(clean_tokenize(data['content'][5083]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [clean_tokenize(row) for row in data.content.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an SMS classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `gensim` & `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        if self.epoch % 10 == 0:\n",
    "            print(f\"Epoch {self.epoch:02d} started...\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model params\n",
    "N_EPOCHS  = 100\n",
    "EMB_DIM   = 300\n",
    "WINDOW    = 20\n",
    "MIN_COUNT = 1\n",
    "SKIP_GRAM = 1\n",
    "\n",
    "gensim_params = dict(\n",
    "    size = EMB_DIM, \n",
    "    sg = SKIP_GRAM,\n",
    "    iter = N_EPOCHS,\n",
    "    window = WINDOW,\n",
    "    min_count = MIN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = Word2Vec(tokenized, **gensim_params, callbacks = [EpochLogger()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.label.values\n",
    "\n",
    "labels_clean = []\n",
    "doc_vecs = []\n",
    "\n",
    "for label, row in zip(labels, tokenized):\n",
    "    try:\n",
    "        doc_vecs.append(get_avg_embedding(row, model))\n",
    "        labels_clean.append(label)\n",
    "    except ZeroDivisionError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the vectors & prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(doc_vecs)\n",
    "\n",
    "y_train = np.array(labels_clean).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize doc embeddings and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_2d = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize = (15, 8))\n",
    "sns.scatterplot(X_train_2d[:, 0], \n",
    "                X_train_2d[:, 1], \n",
    "                hue = ['spam' if i == 1 else 'ham' for i in labels_clean], \n",
    "                alpha = .2)\n",
    "plt.title('Two first components of document-level averaged word vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_01 = keras.Sequential([\n",
    "    keras.layers.Dense(16, input_shape = (300,), activation = 'selu', kernel_initializer = 'lecun_normal'),\n",
    "    keras.layers.AlphaDropout(.2),\n",
    "    keras.layers.Dense(64, activation = 'selu', kernel_initializer = 'lecun_normal'),\n",
    "    keras.layers.AlphaDropout(.2),\n",
    "    keras.layers.Dense(32, activation = 'selu', kernel_initializer = 'lecun_normal'),\n",
    "    keras.layers.AlphaDropout(.2),\n",
    "    keras.layers.Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "clf_01.compile(loss = keras.losses.SparseCategoricalCrossentropy(), \n",
    "               optimizer = keras.optimizers.RMSprop(learning_rate=.001),\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_01.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_01.fit(X_scaled, y_train, epochs = 50, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model loss\n",
    "plt.plot(clf_01.history.history['loss'], label = 'Loss', lw = 1)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message, pred, label in zip(tokenized, clf_01.predict(X_scaled).argmax(axis = 1), y_train):\n",
    "    if pred != label:\n",
    "        print(f'{\" \".join(message)[:80]:90}| Label: {int(label)} Pred: {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pure `keras` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train embeddings simultaneously with the classifier using [Keras `Embedding` layer](https://keras.io/api/layers/core_layers/embedding/).\n",
    "\n",
    "This layer learns embeddings specific for your supervised task. That's different from unsupervised (or self-supervised) approach used in pure Word2Vec. \n",
    "\n",
    "Both approaches might have advantages, depending on your task's context. E.g., [this paper](https://arxiv.org/abs/1804.06323) describes when using pre-trained vs learned embeddings can be beneficial.\n",
    "\n",
    "Please note that the keras model that we train does not perform document-level averaging for embeddings. That's another difference comparing to our previous approach presented in ***Using gensim & keras*** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maxlen \n",
    "MAX_LEN = max([len(row) for row in tokenized])\n",
    "\n",
    "# Get vocab size\n",
    "unique_words = []\n",
    "\n",
    "for row in tokenized:\n",
    "    for word in row:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "          \n",
    "VOCAB_SIZE = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words = VOCAB_SIZE, filters = '')\n",
    "tokenizer.fit_on_texts([' '.join(row) for row in tokenized])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences([' '.join(row) for row in tokenized])\n",
    "padded    = keras.preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_LEN, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare y_train\n",
    "y_train_02 = data.label.values.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_02 = keras.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE, 300, mask_zero = True, input_length = MAX_LEN),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(16, activation = 'selu', kernel_initializer = 'lecun_normal'),\n",
    "    keras.layers.AlphaDropout(.2),\n",
    "    keras.layers.Dense(64, activation = 'selu', kernel_initializer = 'lecun_normal'),\n",
    "    keras.layers.AlphaDropout(.2),\n",
    "    keras.layers.Dense(32, activation = 'selu', kernel_initializer = 'lecun_normal'),\n",
    "    keras.layers.AlphaDropout(.2),\n",
    "    keras.layers.Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "clf_02.compile(loss = keras.losses.SparseCategoricalCrossentropy(), \n",
    "               optimizer = keras.optimizers.RMSprop(learning_rate=.001),\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_02.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_02.fit(padded, y_train_02, epochs = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model loss\n",
    "plt.plot(clf_01.history.history['loss'], label = 'Loss clf_01', lw = 1)\n",
    "plt.plot(clf_02.history.history['loss'], label = 'Loss clf_02', lw = 1)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message, pred, label in zip(tokenized, clf_02.predict(padded).argmax(axis = 1), y_train_02):\n",
    "    if pred != label:\n",
    "        print(f'{\" \".join(message)[:80]:90}| Label: {int(label)} Pred: {pred}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
