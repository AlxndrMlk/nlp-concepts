{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Concepts #5\n",
    "## The World of Sequences -  Recurrent Neural Networks II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple neural network\n",
    "\n",
    "Let's take a simple shallow neural network. We can define it as\n",
    "\n",
    "\n",
    "\n",
    "$$\\Large \\hat{y} = a(W^Tx + b)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a$ is some activation function\n",
    "* $\\hat{y}$ is the output (scalar or vector, depending on $a$)\n",
    "* $W$ is a weight matrix\n",
    "* $x$ is the input vector\n",
    "* $b$ is a bias vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN\n",
    "\n",
    "**RNN** needs a hidden state:\n",
    "\n",
    "$$\\Large h_t = a(W_{xh}^T x_t + W_{hh}^T h_{t-1} + b_h)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a$ is some activation function\n",
    "* $x$ is the input vector\n",
    "* $b_h$ is a hidden layer bias vector\n",
    "* $W_{xh}$ is a weight matrix mapping from feature vector $x \\in R^{D_x}$ to a hidden state $h \\in R^{D_h}$\n",
    "* $W_{hh}$ is a weight matrix mapping from hidden state $h \\in R^{D_h}$ to a hidden state $h \\in R^{D_h}$\n",
    "\n",
    "Then we can compute the output(s):\n",
    "\n",
    "$$\\Large \\hat{y} = a(W_o^Th_t + b_o)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a$ is some activation function\n",
    "* $\\hat{y}$ is the output (scalar or vector, depending on $a$)\n",
    "* $x$ is the input vector\n",
    "* $b_o$ is an output layer bias vector\n",
    "* $W_o$ is an output layer weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "**GRU** introduces two new concepts: **update gate** $z_t$ and **reset gate** $r_t$.\n",
    "\n",
    "Let's define them:\n",
    "\n",
    "$$\\Large z_t = \\sigma(W_{xz}^T x_t + W_{hz}^T h_{t-1} + b_z)$$\n",
    "\n",
    "\n",
    "$$\\Large r_t = \\sigma(W_{xr}^T x_t + W_{hr}^T h_{t-1} + b_r)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\sigma$ is a sigmoid activation function\n",
    "* $x$ is the input vector\n",
    "* $b$ are respective bias vectors\n",
    "* $W$ are respective weight matrices\n",
    "\n",
    "\n",
    "When $z_t$ and $r_t$ are computed, we can compute the **hidden state**:\n",
    "\n",
    "\n",
    "$$\\Large h_t = (1-z_t)\\odot h_{t-1} + z_t \\odot a(W_{xh}^T x_t + W_{hh}^T (r_t \\odot h_{t-1}) + b_h)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\odot$ is element-wise multiplication\n",
    "* $a$ is an activation function (typically $tanh$)\n",
    "\n",
    "\n",
    "Finally, we can compute the output(s):\n",
    "\n",
    "$$\\Large \\hat{y} = a(W_o^Th_t + b_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "**LSTM** is the most complex architecture.\n",
    "\n",
    "It uses a couple of additional concepts, some of them very similar to the ones used in **GRU**.\n",
    "\n",
    "To build an **LSTM**, we'll need:\n",
    "\n",
    "* forget gate $f_t$\n",
    "* input gate (AKA update gate) $i_t$\n",
    "* output gate $o_t$\n",
    "* cell state (AKA extra hidden state) $c_t$\n",
    "* regular hidden state $h_t$\n",
    "\n",
    "Let's define them:\n",
    "\n",
    "$$\\Large f_t = \\sigma(W_{xf}^T x_t + W_{hf}^T h_{t-1} + b_f)$$\n",
    "\n",
    "$$\\Large i_t = \\sigma(W_{xi}^T x_t + W_{hi}^T h_{t-1} + b_i)$$\n",
    "\n",
    "$$\\Large o_t = \\sigma(W_{xo}^T x_t + W_{ho}^T h_{t-1} + b_o)$$\n",
    "\n",
    "$$\\Large c_t = f_t \\odot c_{t-1} + i_t \\odot a(W_{xc}^T x_t + W_{hc}^T h_{t-1} + b_c)$$\n",
    "\n",
    "$$\\Large h_t = o_t \\odot a(c_t)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\odot$ is element-wise multiplication\n",
    "* $a$ is an activation function (typically $tanh$)\n",
    "* $\\sigma$ is a sigmoid activation function\n",
    "* $x$ is the input vector\n",
    "* $b$ are respective bias vectors\n",
    "* $W$ are respective weight matrices\n",
    "\n",
    "\n",
    "We compute the output(s) as usual:\n",
    "\n",
    "$$\\Large \\hat{y} = a(W_o^Th_t + b_o)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
