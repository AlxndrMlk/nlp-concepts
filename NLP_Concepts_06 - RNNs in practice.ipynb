{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Concepts #6 - RNNs in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(text, slice_len=100):\n",
    "    \n",
    "    text_split = text.split(' ')\n",
    "    \n",
    "    n_chunks = int(len(text_split) / slice_len)\n",
    "    current_start_id = 0\n",
    "    \n",
    "    slices = []\n",
    "    \n",
    "    for i in range(n_chunks + 1):\n",
    "        current_slice = text_split[current_start_id:current_start_id + slice_len]\n",
    "        \n",
    "        if len(current_slice) > 0:\n",
    "            slices.append(' '.join(current_slice))\n",
    "        \n",
    "        current_start_id += slice_len\n",
    "        \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print corpora and their lengths\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "    src = nltk.corpus.gutenberg.words(i)\n",
    "    print(i, len(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join and check lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare's \"Macbeth\"\n",
    "shkspr = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
    "shkspr_join = ' '.join(shkspr)\n",
    "\n",
    "len(shkspr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carroll's \"Alice's adventures (...)\"\n",
    "carroll = nltk.corpus.gutenberg.words('carroll-alice.txt')[:23140]\n",
    "carroll_join = ' '.join(carroll)\n",
    "\n",
    "len(carroll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get slices and generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get slices\n",
    "shkspr_slices = get_slices(shkspr_join, 250)\n",
    "carroll_slices = get_slices(carroll_join, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shkspr_slices), len(carroll_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X\n",
    "X = shkspr_slices + carroll_slices\n",
    "\n",
    "# Create y\n",
    "y = np.array([0] * 93 + [1] * 93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=VOCAB_SIZE,\n",
    "    lower=True,  \n",
    "    oov_token=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the toknizer\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train) \n",
    "X_test_tok = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot seq lens\n",
    "seq_lens_train = [len(seq) for seq in X_train_tok]\n",
    "seq_lens_test = [len(seq) for seq in X_test_tok]\n",
    "\n",
    "plt.hist(seq_lens_train, density=True, alpha=.7, label='Train')\n",
    "plt.hist(seq_lens_test, density=True, alpha=.7, label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find maxlen\n",
    "MAXLEN = max([len(x.split(' ')) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "X_train_tok_pad = pad_sequences(X_train_tok, maxlen=MAXLEN, padding='post')\n",
    "X_test_tok_pad = pad_sequences(X_test_tok, maxlen=MAXLEN, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val, epochs=30, lr=1e-4, verbose=2):\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = tf.keras.optimizers.Adam(lr),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    # Callbacks\n",
    "    early = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Time it\n",
    "    start = time.time()\n",
    "    \n",
    "    # Fit \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       validation_data = (X_val, y_val),\n",
    "                       callbacks = [early],\n",
    "                       epochs = epochs,\n",
    "                       verbose = verbose)\n",
    "    \n",
    "    # Time it\n",
    "    training_time = time.time() - start\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='val')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    print(f'Val. accuracy: {acc}')\n",
    "    print(f'Training time: {training_time:.02f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = VOCAB_SIZE,\n",
    "        output_dim = 100,\n",
    "        mask_zero = True,\n",
    "        input_length = MAXLEN),\n",
    "    \n",
    "    tf.keras.layers.LSTM(64),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(model, X_train_tok_pad, y_train, X_test_tok_pad, y_test, verbose=0, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a deeper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = VOCAB_SIZE,\n",
    "        output_dim = 100,\n",
    "        mask_zero = True,\n",
    "        input_length = MAXLEN),\n",
    "    \n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(model_2, X_train_tok_pad, y_train, X_test_tok_pad, y_test, verbose=0, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a bi-directional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = VOCAB_SIZE,\n",
    "        output_dim = 100,\n",
    "        mask_zero = True,\n",
    "        input_length = MAXLEN),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(model_3, X_train_tok_pad, y_train, X_test_tok_pad, y_test, verbose=0, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a deep bi-directional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = VOCAB_SIZE,\n",
    "        output_dim = 100,\n",
    "        mask_zero = True,\n",
    "        input_length = MAXLEN),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(model_4, X_train_tok_pad, y_train, X_test_tok_pad, y_test, verbose=0, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long distance dependencies - `SimpleRNN()` vs `LSTM()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment:\n",
    "\n",
    "We will put a keyword (***Fußbodenheizung*** - a German word for (under)floor heating) at the beginnig of a random sequence. We'll manipulate sequence length and check how it affects performance of `SimpleRNN` and `LSTM` in a classification task.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://www.heizsparer.de/wp-content/uploads/images/estrich-fussbodenheizung-wolfilser-adobestock.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD = 'fußbodenheizung'\n",
    "LENGTHS = [10, 30, 50, 200]\n",
    "VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fußbodenheizung' in ' '.join(carroll).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(length, n_examples):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        class_ = np.random.choice([0, 1])\n",
    "        \n",
    "        if class_ == 1:\n",
    "            row = np.array([-1] + list(np.random.choice(np.arange(0, 1, .01), length - 1)))\n",
    "        elif class_ == 0:\n",
    "            row = np.random.choice(np.arange(0, 1, .01), length)\n",
    "            \n",
    "        X.append(row)\n",
    "        y.append(class_)\n",
    "        \n",
    "    return np.array(X)[:, :, np.newaxis], np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(rnn_type, len_):\n",
    "    \n",
    "    if rnn_type == 'rnn':\n",
    "        rnn_layer = tf.keras.layers.SimpleRNN\n",
    "    elif rnn_type == 'lstm':\n",
    "        rnn_layer = tf.keras.layers.LSTM\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "\n",
    "        rnn_layer(64, input_shape=(len_, 1), return_sequences=True),\n",
    "        rnn_layer(128),\n",
    "        \n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dropout(.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for len_ in LENGTHS:\n",
    "    \n",
    "    # Prep data\n",
    "    print(f'Buidling dataset of length {len_}')\n",
    "    X, y = build_dataset(len_, 200)\n",
    "    \n",
    "    # Train test split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "    \n",
    "    # Build models\n",
    "    rnn_model = build_model('rnn', len_)\n",
    "    lstm_model = build_model('lstm', len_)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(f'\\nRNN for {len_}')\n",
    "    train_and_evaluate(rnn_model, X_train, y_train, X_test, y_test, verbose=0, epochs=30)\n",
    "    \n",
    "    print(f'\\nLSTM for {len_}')\n",
    "    train_and_evaluate(lstm_model, X_train, y_train, X_test, y_test, verbose=0, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-dl-env]",
   "language": "python",
   "name": "conda-env-nlp-dl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
